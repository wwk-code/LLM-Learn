{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1ea03a-cc69-45b0-80d3-664e48ca6831",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-recipes/blob/main/recipes/use_cases/RAG/HelloLlamaCloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## This demo app shows:\n",
    "* How to run Llama 3.1 in the cloud hosted on Replicate\n",
    "* How to use LangChain to ask Llama general questions and follow up questions\n",
    "* How to use LangChain to load a recent web page - Hugging Face's [blog post on Llama 3.1](https://huggingface.co/blog/llama31) - and chat about it. This is the well known RAG (Retrieval Augmented Generation) method to let LLM such as Llama 3 be able to answer questions about the data not publicly available when Llama 3 was trained, or about your own data. RAG is one way to prevent LLM's hallucination\n",
    "\n",
    "**Note** We will be using [Replicate](https://replicate.com/meta/meta-llama-3.1-405b-instruct) to run the examples here. You will need to first sign in with Replicate with your github account, then create a free API token [here](https://replicate.com/account/api-tokens) that you can use for a while. You can also use other Llama 3.1 cloud providers such as [Groq](https://console.groq.com/), [Together](https://api.together.xyz/playground/language/meta-llama/Llama-3-8b-hf), or [Anyscale](https://app.endpoints.anyscale.com/playground) - see Section 2 of the Getting to Know Llama [notebook](https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Getting_to_know_Llama.ipynb) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dde626",
   "metadata": {},
   "source": [
    "Let's start by installing the necessary packages:\n",
    "- sentence-transformers for text embeddings\n",
    "- FAISS gives us database capabilities \n",
    "- LangChain provides necessary RAG tools for this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c608df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /root/anaconda3/lib/python3.12/site-packages (0.3.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.13)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (0.1.137)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /root/anaconda3/lib/python3.12/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /root/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /root/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /root/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /root/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /root/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /root/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /root/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /root/anaconda3/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in /root/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /root/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /root/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /root/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence-transformers in /root/anaconda3/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /root/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /root/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /root/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /root/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /root/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /root/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.25.2)\n",
      "Requirement already satisfied: Pillow in /root/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: sympy in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /root/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/anaconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /root/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /root/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: faiss-cpu in /root/anaconda3/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /root/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /root/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (23.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: bs4 in /root/anaconda3/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /root/anaconda3/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /root/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: replicate in /root/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: httpx<1,>=0.21.0 in /root/anaconda3/lib/python3.12/site-packages (from replicate) (0.27.2)\n",
      "Requirement already satisfied: packaging in /root/anaconda3/lib/python3.12/site-packages (from replicate) (23.2)\n",
      "Requirement already satisfied: pydantic>1.10.7 in /root/anaconda3/lib/python3.12/site-packages (from replicate) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /root/anaconda3/lib/python3.12/site-packages (from replicate) (4.8.0)\n",
      "Requirement already satisfied: anyio in /root/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.21.0->replicate) (4.2.0)\n",
      "Requirement already satisfied: certifi in /root/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.21.0->replicate) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /root/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.21.0->replicate) (1.0.6)\n",
      "Requirement already satisfied: idna in /root/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.21.0->replicate) (3.7)\n",
      "Requirement already satisfied: sniffio in /root/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.21.0->replicate) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/anaconda3/lib/python3.12/site-packages (from pydantic>1.10.7->replicate) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /root/anaconda3/lib/python3.12/site-packages (from pydantic>1.10.7->replicate) (2.23.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install bs4\n",
    "!pip install replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c5546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8870c1",
   "metadata": {},
   "source": [
    "Next we call the Llama 3.1 405b chat model from Replicate. You can also use Llama 3 8B or 70B model by replacing the `model` name with the respective model URL(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad536adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "\n",
    "model_kwargs={\"temperature\": 0.75, \"max_length\": 500, \"top_p\": 1}\n",
    "\n",
    "\n",
    "llm = Replicate(\n",
    "    model=\"meta/meta-llama-3.1-405b-instruct\",\n",
    "    model_kwargs={\"temperature\": 0.75, \"max_length\": 500, \"top_p\": 1},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd207c80",
   "metadata": {},
   "source": [
    "With the model set up, you are now ready to ask some questions. Here is an example of the simplest way to ask the model some general questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "493a7148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm excited to share with you what's new with Llama 3! Llama 3 is an updated version of the AI model that I'm a part of, and it brings several improvements and features. Here are some of the notable updates:\n",
      "\n",
      "1. **Improved accuracy and understanding**: Llama 3 has been trained on a massive dataset of text from various sources, which enables it to better comprehend and respond to a wide range of questions and topics.\n",
      "2. **Enhanced contextual understanding**: Llama 3 can now better understand the context of a conversation, allowing it to provide more accurate and relevant responses.\n",
      "3. **Increased knowledge base**: Llama 3 has been updated with a vast amount of new knowledge, including but not limited to, recent events, scientific breakthroughs, and cultural phenomena.\n",
      "4. **Better handling of idioms and colloquialisms**: Llama 3 can now better understand and respond to idiomatic expressions, colloquialisms, and figurative language.\n",
      "5. **More engaging and conversational tone**: Llama 3 is designed to be more engaging and conversational, making interactions feel more natural and human-like.\n",
      "6. **Improved emotional intelligence**: Llama 3 can now better recognize and respond to emotions, empathizing with users and providing more supportive and understanding responses.\n",
      "7. **Multilingual support**: Llama 3 can understand and respond in multiple languages, making it more accessible to users worldwide.\n",
      "8. **Enhanced creative capabilities**: Llama 3 can now generate more creative and original content, including stories, poems, and dialogues.\n",
      "\n",
      "These updates and features make Llama 3 an even more powerful and helpful tool for users. I'm excited to see how it can assist and provide value to people!\n"
     ]
    }
   ],
   "source": [
    "# question = \"who wrote the book Innovator's dilemma?\"\n",
    "question = \"What's new with Llama 3?\"\n",
    "answer = llm.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315f000",
   "metadata": {},
   "source": [
    "We will then try to follow up the response with a question asking for more information on the book. \n",
    "\n",
    "Since the chat history is not passed on Llama doesn't have the context and doesn't know this is more about the book thus it treats this as new query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5c8676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a helpful assistant, my purpose is to assist and provide useful information to users like you. I can help with a wide range of topics, from answering simple questions to providing more in-depth information on various subjects.\n",
      "\n",
      "Here are some examples of what I can help with:\n",
      "\n",
      "1. **General knowledge**: I can provide information on history, science, technology, literature, and more.\n",
      "2. **Language translation**: I can translate text from one language to another, including popular languages such as Spanish, French, German, Chinese, and many more.\n",
      "3. **Writing and proofreading**: I can help with writing and proofreading tasks, such as suggesting alternative phrases, correcting grammar and spelling errors, and providing feedback on clarity and coherence.\n",
      "4. **Math and calculations**: I can perform mathematical calculations, from simple arithmetic to more complex equations, and provide explanations for mathematical concepts.\n",
      "5. **Conversation and chat**: I can engage in natural-sounding conversations, using context and understanding to respond to questions and statements.\n",
      "6. **Jokes and humor**: I can share jokes and try to be funny (though humor is subjective, so no promises I'll always succeed!)\n",
      "7. **Word games and puzzles**: I can play games like Hangman, Word Jumble, and Word Scramble, and provide solutions to word puzzles and brain teasers.\n",
      "8. **Generating text**: I can create text based on a prompt or topic, and can even help with writing tasks such as composing emails or creating articles.\n",
      "\n",
      "These are just a few examples of what I can do. If you have a specific question or task in mind, feel free to ask me and I'll do my best to help!\n"
     ]
    }
   ],
   "source": [
    "# chat history not passed so Llama doesn't have the context and doesn't know this is more about the book\n",
    "followup = \"tell me more\"\n",
    "followup_answer = llm.invoke(followup)\n",
    "print(followup_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeaffc7",
   "metadata": {},
   "source": [
    "To get around this we will need to provide the model with history of the chat. \n",
    "\n",
    "To do this, we will use  [`ConversationBufferMemory`](https://python.langchain.com/docs/modules/memory/types/buffer) to pass the chat history to the model and give it the capability to handle follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5428ca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6468/2439804699.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_6468/2439804699.py:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "# using ConversationBufferMemory to pass memory (chat history) for follow up questions\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9af5f",
   "metadata": {},
   "source": [
    "Once this is set up, let us repeat the steps from before and ask the model a simple question.\n",
    "\n",
    "Then we pass the question and answer back into the model for context along with the follow up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baee2d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book \"The Innovator's Dilemma\" was written by Clayton M. Christensen, an American business consultant and academic. It was first published in 1997 and has since become a classic in the field of business innovation and disruption. Christensen was a Harvard Business School professor and is widely recognized for his work on innovation and disruption, and this book is considered one of his most influential works. Would you like to know more about the book's main ideas or Christensen's other works?\n"
     ]
    }
   ],
   "source": [
    "# restart from the original question\n",
    "answer = conversation.predict(input=question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c7d67a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book \"The Innovator's Dilemma\" by Clayton M. Christensen is a seminal work that explores the concept of disruption in the business world. Christensen argues that well-established companies can fail when they are faced with a new, innovative technology or business model that disrupts their existing market. This is because these companies often prioritize short-term profits and existing customer needs over investing in new, unproven technologies that may cannibalize their existing business.\n",
      "\n",
      "Christensen identifies two types of innovations: sustaining innovations, which improve existing products or services, and disruptive innovations, which create new markets or disrupt existing ones. He argues that companies should focus on creating separate teams or organizations to pursue disruptive innovations, as these efforts often require different resources, processes, and cultures than the core business.\n",
      "\n",
      "One of the key examples Christensen uses in the book is the story of the hard disk drive industry, where companies like Seagate and Western Digital were disrupted by new entrants like IBM and Hitachi, which developed smaller, more efficient disk drives that created new markets and eventually replaced the existing ones.\n",
      "\n",
      "Christensen's work has had a significant impact on the way companies think about innovation and disruption, and his ideas have been widely adopted across industries. Would you like to know more about how companies can apply these ideas in practice or about Christensen's other works, such as \"The Innovator's Solution\" or \"Competing Against Luck\"?\n"
     ]
    }
   ],
   "source": [
    "# pass context (previous question and answer) along with the follow up \"tell me more\" to Llama who now knows more of what\n",
    "memory.save_context({\"input\": question},\n",
    "                    {\"output\": answer})\n",
    "followup_answer = conversation.predict(input=followup)\n",
    "print(followup_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc436163",
   "metadata": {},
   "source": [
    "Next, let's explore using Llama 3.1 to answer questions using documents for context. \n",
    "This gives us the ability to update Llama 3.1's knowledge thus giving it better context without needing to finetune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5303d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader([\"https://huggingface.co/blog/llama3\"])\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6cd390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\", 'description': 'We’re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWelcome Llama 3 - Meta\\'s new open LLM\\n\\n\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\tSpaces\\n\\n\\t\\t\\t\\t\\tPosts\\n\\n\\t\\t\\t\\t\\tDocs\\n\\n\\n\\n\\n\\t\\t\\tSolutions\\n\\t\\t\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tBack to Articles\\n\\n\\n\\n\\n\\n\\t\\tWelcome Llama 3 - Meta’s new open LLM\\n\\t\\n\\n\\nPublished\\n\\t\\t\\t\\tApril 18, 2024\\nUpdate on GitHub\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t275\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+269\\n\\n\\n\\nphilschmid\\nPhilipp Schmid\\n\\n\\n\\n\\n\\nosanseviero\\nOmar Sanseviero\\n\\n\\n\\n\\n\\npcuenq\\nPedro Cuenca\\n\\n\\n\\n\\n\\nybelkada\\nYounes Belkada\\n\\n\\n\\n\\n\\nlvwerra\\nLeandro von Werra\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\n\\nTable of contents\\n\\nWhat’s new with Llama 3?\\n\\nLlama 3 evaluation\\n\\nHow to prompt Llama 3\\n\\nDemo\\n\\nUsing 🤗\\xa0Transformers\\n\\nInference Integrations\\nIntegration with Inference Endpoints\\n\\nIntegration with Google Cloud\\n\\nIntegration with Amazon SageMaker\\n\\n\\nFine-tuning with 🤗\\xa0TRL\\n\\nAdditional Resources\\n\\nAcknowledgments\\n\\n\\n\\n\\n\\n\\n\\t\\tIntroduction\\n\\t\\n\\nMeta’s Llama 3, the next iteration of the open-access Llama family, is now released and available at Hugging Face. It\\'s great to see Meta continuing its commitment to open AI, and we’re excited to fully support the launch with comprehensive integration in the Hugging Face ecosystem.\\nLlama 3 comes in two sizes: 8B for efficient deployment and development on consumer-size GPU, and 70B for large-scale AI native applications. Both come in base and instruction-tuned variants. In addition to the 4 models, a new version of Llama Guard was fine-tuned on Llama 3 8B and is released as Llama Guard 2 (safety fine-tune).\\nWe’ve collaborated with Meta to ensure the best integration into the Hugging Face ecosystem. You can find all 5 open-access models (2 base models, 2 fine-tuned & Llama Guard) on the Hub. Among the features and integrations being released, we have:\\n\\nModels on the Hub, with their model cards and licenses\\n🤗 Transformers integration\\nHugging Chat integration for Meta Llama 3 70b\\nInference Integration into Inference Endpoints, Google Cloud & Amazon SageMaker\\nAn example of fine-tuning Llama 3 8B on a single GPU with 🤗\\xa0TRL\\n\\n\\n\\n\\n\\n\\n\\t\\tTable of contents\\n\\t\\n\\n\\nWhat’s new with Llama 3?\\nLlama 3 evaluation\\nHow to prompt Llama 3\\nDemo\\nUsing 🤗\\xa0Transformers\\nInference Integrations\\nFine-tuning with 🤗\\xa0TRL\\nAdditional Resources\\nAcknowledgments\\n\\n\\n\\n\\n\\n\\n\\t\\tWhat’s new with Llama 3?\\n\\t\\n\\nThe Llama 3 release introduces 4 new open LLM models by Meta based on the Llama 2 architecture. They come in two sizes: 8B and 70B parameters, each with base (pre-trained) and instruct-tuned versions. All the variants can be run on various types of consumer hardware and have a context length of 8K tokens. \\n\\nMeta-Llama-3-8b: Base 8B model\\nMeta-Llama-3-8b-instruct: Instruct fine-tuned version of the base 8b model\\nMeta-Llama-3-70b: Base 70B model\\nMeta-Llama-3-70b-instruct: Instruct fine-tuned version of the base 70b model\\n\\nIn addition to these 4 base models, Llama Guard 2 was also released. Fine-tuned on Llama 3 8B, it’s the latest iteration in the Llama Guard family. Llama Guard 2, built for production use cases, is designed to classify LLM inputs (prompts) as well as LLM responses in order to detect content that would be considered unsafe in a risk taxonomy.\\nA big change in Llama 3 compared to Llama 2 is the use of a new tokenizer that expands the vocabulary size to 128,256 (from 32K tokens in the previous version). This larger vocabulary can encode text more efficiently (both for input and output) and potentially yield stronger multilingualism. This comes at a cost, though: the embedding input and output matrices are larger, which accounts for a good portion of the parameter count increase of the small model: it goes from 7B in Llama 2 to 8B in Llama 3. In addition, the 8B version of the model now uses Grouped-Query Attention (GQA), which is an efficient representation that should help with longer contexts. \\nThe Llama 3 models were trained ~8x more data on over 15 trillion tokens on a new mix of publicly available online data on two clusters with 24,000 GPUs. We don’t know the exact details of the training mix, and we can only guess that bigger and more careful data curation was a big factor in the improved performance. Llama 3 Instruct has been optimized for dialogue applications and was trained on over 10 Million human-annotated data samples with combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct policy optimization (DPO). \\nRegarding the licensing terms, Llama 3 comes with a permissive license that allows redistribution, fine-tuning, and derivative works. The requirement for explicit attribution is new in the Llama 3 license and was not present in Llama 2. Derived models, for instance, need to include \"Llama 3\" at the beginning of their name, and you also need to mention \"Built with Meta Llama 3\" in derivative works or services. For full details, please make sure to read the official license.\\n\\n\\n\\n\\n\\n\\t\\tLlama 3 evaluation\\n\\t\\n\\nHere, you can see a list of models and their Open LLM Leaderboard scores. This is not a comprehensive list and we encourage you to look at the full leaderboard. Note that the LLM Leaderboard is specially useful to evaluate pre-trained models, as there are other benchmarks specific to conversational models. \\n\\n\\n\\nModel\\nLicense\\nPretraining length [tokens]\\nLeaderboard score\\n\\n\\n\\nMPT-7B\\nApache 2.0\\n1,000B\\n5.98\\n\\n\\nFalcon-7B\\nApache 2.0\\n1,500B\\n5.1\\n\\n\\nLlama-2-7B\\nLlama 2 license\\n2T\\n8.72\\n\\n\\nQwen 2 7B\\nApache 2.0\\n?\\n23.66\\n\\n\\nLlama-3-8B\\nLlama 3 license\\n15T\\n13.41\\n\\n\\nLlama-2-13B\\nLlama 2 license\\n2T\\n10.99\\n\\n\\nFalcon-40B\\nApache 2.0\\n1,000B\\n11.33\\n\\n\\nFalcon-40B\\nApache 2.0\\n1,000B\\n11.33\\n\\n\\nLlama-2-70B\\nLlama 2 license\\n2T\\n18.25\\n\\n\\nLlama-3-70B\\nLlama 3 license\\n15T\\n26.37\\n\\n\\nMixtral 8x22B\\nApache 2\\n?\\n25.49\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tHow to prompt Llama 3\\n\\t\\n\\nThe base models have no prompt format. Like other base models, they can be used to continue an input sequence with a plausible continuation or for zero-shot/few-shot inference. They are also a great foundation for fine-tuning your own use cases. The Instruct versions use the following conversation structure:\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{{ model_answer_1 }}<|eot_id|>\\n\\nThis format has to be exactly reproduced for effective use. We’ll later show how easy it is to reproduce the instruct prompt with the chat template available in transformers. \\n\\n\\n\\n\\n\\n\\t\\tDemo\\n\\t\\n\\nYou can chat with the Llama 3 70B instruct on Hugging Chat! Check out the link here: https://huggingface.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct\\n\\n\\n\\n\\n\\n\\t\\tUsing 🤗\\xa0Transformers\\n\\t\\n\\nWith Transformers\\xa0release 4.40, you can use Llama 3 and leverage all the tools within the Hugging Face ecosystem, such as:\\n\\ntraining and inference scripts and examples\\nsafe file format (safetensors)\\nintegrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning), and Flash Attention 2\\nutilities and helpers to run generation with the model\\nmechanisms to export the models to deploy\\n\\nIn addition, Llama 3 models are compatible with torch.compile() with CUDA graphs, giving them a ~4x speedup at inference time!\\nTo use Llama 3 models with transformers, make sure to install a recent version of transformers:\\npip install --upgrade transformers\\n\\nThe following snippet shows how to use Llama-3-8b-instruct with transformers. It requires about 16 GB of RAM, which includes consumer GPUs such as 3090 or 4090.\\nfrom transformers import pipeline\\nimport torch\\n\\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\\n\\npipe = pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\\n    device=\"cuda\",\\n)\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\\n]\\n\\nterminators = [\\n    pipe.tokenizer.eos_token_id,\\n    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\\n]\\n\\noutputs = pipe(\\n    messages,\\n    max_new_tokens=256,\\n    eos_token_id=terminators,\\n    do_sample=True,\\n    temperature=0.6,\\n    top_p=0.9,\\n)\\nassistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\\nprint(assistant_response)\\n\\n\\nArrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas! Me be here to swab the decks o\\' yer mind with me trusty responses, savvy? I be ready to hoist the Jolly Roger and set sail fer a swashbucklin\\' good time, matey! So, what be bringin\\' ye to these fair waters?\\n\\nA couple of details:\\n\\nWe loaded the model in bfloat16. This is the type used by the original checkpoint published by Meta, so it’s the recommended way to run to ensure the best precision or to conduct evaluations. For real world use, it’s also safe to use float16, which may be faster depending on your hardware.\\nAssistant responses may end with the special token <|eot_id|>, but we must also stop generation if the regular EOS token is found. We can stop generation early by providing a list of terminators in the eos_token_id parameter.\\nWe used the default sampling parameters (temperature and top_p) taken from the original meta codebase. We haven’t had time to conduct extensive tests yet, feel free to explore!\\n\\nYou can also automatically quantize the model, loading it in 8-bit or even 4-bit mode. 4-bit loading takes about 7 GB of memory to run, making it compatible with a lot of consumer cards and all the GPUs in Google Colab. This is how you’d load the generation pipeline in 4-bit:\\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\\n        \"torch_dtype\": torch.float16,\\n        \"quantization_config\": {\"load_in_4bit\": True},\\n        \"low_cpu_mem_usage\": True,\\n    },\\n)\\n\\nFor more details on using the models with transformers, please check the model cards.\\n\\n\\n\\n\\n\\n\\t\\tInference Integrations\\n\\t\\n\\nIn this section, we’ll go through different approaches to running inference of the Llama 3 models. Before using these models, make sure you have requested access to one of the models in the official\\xa0Meta Llama 3\\xa0repositories.\\n\\n\\n\\n\\n\\n\\t\\tIntegration with Inference Endpoints\\n\\t\\n\\nYou can deploy Llama 3 on Hugging Face\\'s\\xa0Inference Endpoints, which uses Text Generation Inference as the backend. Text Generation Inference\\xa0is a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing.\\nTo deploy Llama 3, go to the\\xa0model page\\xa0and click on the\\xa0Deploy -> Inference Endpoints widget. You can learn more about Deploying LLMs with Hugging Face Inference Endpoints in a previous blog post. Inference Endpoints supports Messages API through Text Generation Inference, which allows you to switch from another closed model to an open one by simply changing the URL.\\nfrom openai import OpenAI\\n\\n# initialize the client but point it to TGI\\nclient = OpenAI(\\n    base_url=\"<ENDPOINT_URL>\" + \"/v1/\",  # replace with your endpoint url\\n    api_key=\"<HF_API_TOKEN>\",  # replace with your token\\n)\\nchat_completion = client.chat.completions.create(\\n    model=\"tgi\",\\n    messages=[\\n        {\"role\": \"user\", \"content\": \"Why is open-source software important?\"},\\n    ],\\n    stream=True,\\n    max_tokens=500\\n)\\n\\n# iterate and print stream\\nfor message in chat_completion:\\n    print(message.choices[0].delta.content, end=\"\")\\n\\n\\n\\n\\n\\n\\n\\t\\tIntegration with Google Cloud\\n\\t\\n\\nYou can deploy Llama 3 on Google Cloud through Vertex AI or Google Kubernetes Engine (GKE), using Text Generation Inference. \\nTo deploy the Llama 3 model from Hugging Face, go to the\\xa0model page\\xa0and click on Deploy -> Google Cloud. This will bring you to the Google Cloud Console, where you can 1-click deploy Llama 3 on Vertex AI or GKE.\\n\\n\\n\\n\\n\\n\\t\\tIntegration with Amazon SageMaker\\n\\t\\n\\nYou can deploy and train Llama 3 on Amazon SageMaker through AWS Jumpstart or using the Hugging Face LLM Container. \\nTo deploy the Llama 3 model from Hugging Face, go to the\\xa0model page\\xa0and click on Deploy -> Amazon SageMaker. This will display a code snippet you can copy and execute in your environment. Amazon SageMaker will now create a dedicated inference endpoint you can use to send requests. \\n\\n\\n\\n\\n\\n\\t\\tFine-tuning with 🤗\\xa0TRL\\n\\t\\n\\nTraining LLMs can be technically and computationally challenging. In this section, we’ll look at the tools available in the Hugging Face ecosystem to efficiently train Llama 3 on consumer-size GPUs. Below is an example command to fine-tune Llama 3 on the No Robots dataset. We use 4-bit quantization, and QLoRA and TRL’s SFTTrainer will automatically format the dataset into chatml format. Let’s get started!\\nFirst, install the latest version of 🤗 TRL. \\npip install -U transformers trl accelerate\\n\\nIf you just want to chat with the model in the terminal you can use the chat command of the TRL CLI (for more info see the docs):\\ntrl chat \\\\\\n--model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct \\\\\\n--device cuda \\\\\\n--eos_tokens \"<|end_of_text|>,<|eod_id|>\"\\n\\nYou can also use TRL CLI to supervise fine-tuning (SFT) Llama 3 on your own, custom dataset. Use the trl sft command and pass your training arguments as CLI argument. Make sure you are logged in and have access the Llama 3 checkpoint. You can do this with huggingface-cli login.\\ntrl sft \\\\\\n--model_name_or_path meta-llama/Meta-Llama-3-8B \\\\\\n--dataset_name HuggingFaceH4/no_robots \\\\\\n--learning_rate 0.0001 \\\\\\n--per_device_train_batch_size 4 \\\\\\n--max_seq_length 2048 \\\\\\n--output_dir ./llama3-sft \\\\\\n--use_peft \\\\\\n--load_in_4bit \\\\\\n--log_with wandb \\\\\\n--gradient_checkpointing \\\\\\n--logging_steps 10\\n\\nThis will run the fine-tuning from your terminal and takes about 4 hours to train on a single A10G, but can be easily parallelized by tweaking --num_processes to the number of GPUs you have available.\\nNote: You can also replace the CLI arguments with a yaml file. Learn more about the TRL CLI here. \\n\\n\\n\\n\\n\\n\\t\\tAdditional Resources\\n\\t\\n\\n\\nModels on the Hub\\nOpen LLM Leaderboard\\nChat demo on Hugging Chat\\nMeta Blog\\nGoogle Cloud Vertex AI model garden\\n\\n\\n\\n\\n\\n\\n\\t\\tAcknowledgments\\n\\t\\n\\nReleasing such models with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including\\n\\nClémentine Fourrier, Nathan Habib,\\xa0and\\xa0Eleuther Evaluation Harness\\xa0for LLM evaluations\\nOlivier Dehaene and Nicolas Patry for Text Generation Inference Support\\nArthur Zucker and Lysandre Debut for adding Llama 3 support in transformers and tokenizers\\nNathan Sarrazin, Victor Mustar, and Kevin Cathaly for making Llama 3 available in Hugging Chat.\\nYuvraj Sharma for the Gradio demo.\\nXenova and Vaibhav Srivastav for debugging and experimentation with quantization and prompt templates.\\nBrigitte Tousignant, Florent Daudens, Morgan Funtowicz, and Simon Brandeis for different items during the launch!\\nThank you to the whole Meta team, including Samuel Selvan, Eleonora Presani, Hamid Shojanazeri, Azadeh Yazdan, Aiman Farooq, Ruan Silva, Ashley Gabriel, Eissa Jamil, Binh Tang, Matthias Reso, Lovish Madaan, Joe Spisak, and Sergey Edunov.\\n\\nThank you to the Meta Team for releasing Llama 3 and making it available to the open-source AI community!\\n\\nMore Articles from our Blog\\n\\n\\nUniversal Assisted Generation: Faster Decoding with Any Assistant Model\\n\\n\\t\\t\\t\\tBy\\xa0\\ndanielkorat\\n\\n\\nOctober 29, 2024\\n\\nguest\\n•\\n\\n\\n\\t\\t\\t\\t36\\n\\nScaling AI-based Data Processing with Hugging Face + Dask\\n\\n\\t\\t\\t\\tBy\\xa0\\nscj13\\n\\n\\nOctober 9, 2024\\n•\\n\\n\\n\\t\\t\\t\\t22\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t275\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+263\\n\\nCompany\\n© Hugging Face\\nTOS\\nPrivacy\\nAbout\\nJobs\\n\\nWebsite\\nModels\\nDatasets\\nSpaces\\nPricing\\nDocs\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8268e",
   "metadata": {},
   "source": [
    "We need to store our document in a vector store. There are more than 30 vector stores (DBs) supported by LangChain. \n",
    "For this example we will use [FAISS](https://github.com/facebookresearch/faiss), a popular open source vector store by Facebook.\n",
    "For other vector stores especially if you need to store a large amount of data - see [here](https://python.langchain.com/docs/integrations/vectorstores).\n",
    "\n",
    "We will also import the HuggingFaceEmbeddings and RecursiveCharacterTextSplitter to assist in storing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eecb6a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6468/3116357007.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
      "/root/anaconda3/envs/llama/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Split the document into chunks with a specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Store the document into a vector store with a specific embedding model\n",
    "vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4a17c",
   "metadata": {},
   "source": [
    "To store the documents, we will need to split them into chunks using [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) and create vector representations of these chunks using [`HuggingFaceEmbeddings`](https://www.google.com/search?q=langchain+hugging+face+embeddings&sca_esv=572890011&ei=ARUoZaH4LuumptQP48ah2Ac&oq=langchian+hugg&gs_lp=Egxnd3Mtd2l6LXNlcnAiDmxhbmdjaGlhbiBodWdnKgIIADIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCkjeHlC5Cli5D3ABeAGQAQCYAV6gAb4CqgEBNLgBAcgBAPgBAcICChAAGEcY1gQYsAPiAwQYACBBiAYBkAYI&sclient=gws-wiz-serp) on them before storing them into our vector database. \n",
    "\n",
    "In general, you should use larger chuck sizes for highly structured text such as code and smaller size for less structured text. You may need to experiment with different chunk sizes and overlap values to find out the best numbers.\n",
    "\n",
    "We then use `RetrievalQA` to retrieve the documents from the vector database and give the model more context on Llama 3.1, thereby increasing its knowledge. 3.1 also really shines with the new 128k context!\n",
    "\n",
    "For each question, LangChain performs a semantic similarity search of it in the vector db, then passes the search results as the context to Llama to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00e3f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6468/3950371262.py:10: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, several things are new with Llama 3:\n",
      "\n",
      "1. Four new open LLM models by Meta based on the Llama 2 architecture, with 8B and 70B parameters and base and instruct-tuned versions.\n",
      "2. A new tokenizer that expands the vocabulary size to 128,256 tokens (from 32K tokens in Llama 2).\n",
      "3. A larger vocabulary that can encode text more efficiently and potentially yield stronger multilingualism.\n",
      "4. A new version of Llama Guard (Llama Guard 2) that was fine-tuned on Llama 3 8B for safety.\n",
      "\n",
      "These changes aim to improve the performance and capabilities of Llama 3 compared to its predecessor, Llama 2.\n"
     ]
    }
   ],
   "source": [
    "# use LangChain's RetrievalQA, to associate Llama 3 with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "question = \"What's new with Llama 3?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63769a",
   "metadata": {},
   "source": [
    "Now, lets bring it all together by incorporating follow up questions.\n",
    "\n",
    "First we ask a follow up questions without giving the model context of the previous conversation. \n",
    "Without this context, the answer we get does not relate to our original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53f27473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough information to provide a specific answer about the architecture of Llama 3 beyond what is mentioned in the text, such as the use of Grouped-Query Attention (GQA) in the 8B version. If you are looking for more detailed architectural information, it is not provided in the given context.\n"
     ]
    }
   ],
   "source": [
    "# no context passed so Llama 3 doesn't have enough context to answer so it lets its imagination go wild\n",
    "result = qa_chain({\"query\": \"Based on what architecture?\"})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833221c0",
   "metadata": {},
   "source": [
    "As we did before, let us use the `ConversationalRetrievalChain` package to give the model context of our previous question so we can add follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "743644a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ConversationalRetrievalChain to pass chat history for follow up questions\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c3d1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3 introduces several new features and improvements, including:\n",
      "\n",
      "1. Four new open LLM models with 8B and 70B parameters, each with base and instruct-tuned versions.\n",
      "2. A new tokenizer that expands the vocabulary size to 128,256, allowing for more efficient text encoding and potentially stronger multilingualism.\n",
      "3. A larger context length of 8K tokens.\n",
      "4. The release of Llama Guard 2, a safety fine-tune version of Llama Guard that was fine-tuned on Llama 3 8B.\n",
      "\n",
      "These changes aim to improve the performance and versatility of the Llama model, while also making it more accessible and user-friendly.\n"
     ]
    }
   ],
   "source": [
    "# let's ask the original question What's new with Llama 3?\" again\n",
    "result = chat_chain({\"question\": question, \"chat_history\": []})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f36f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b17f08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3 is based on the Llama 2 architecture.\n"
     ]
    }
   ],
   "source": [
    "# this time we pass chat history along with the follow up so good things should happen\n",
    "chat_history = [(question, result[\"answer\"])]\n",
    "followup = \"Based on what architecture?\"\n",
    "followup_answer = chat_chain({\"question\": followup, \"chat_history\": chat_history})\n",
    "print(followup_answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95d22347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size in Llama 3 is 128,256. This is an increase from the previous version, Llama 2, which had a vocabulary size of 32,000 tokens (32K). The larger vocabulary in Llama 3 allows for more efficient encoding of text and potentially stronger multilingual capabilities.\n"
     ]
    }
   ],
   "source": [
    "# further follow ups can be made possible by updating chat_history like this:\n",
    "chat_history.append((followup, followup_answer[\"answer\"]))\n",
    "more_followup = \"What changes in vocabulary size?\"\n",
    "more_followup_answer = chat_chain({\"question\": more_followup, \"chat_history\": chat_history})\n",
    "print(more_followup_answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daf288-bfca-4853-b153-0d8c73412804",
   "metadata": {},
   "source": [
    "**Note:** If results can get cut off, you can set \"max_new_tokens\" in the Replicate call above to a larger number (like shown below) to avoid the cut off.\n",
    "\n",
    "```python\n",
    "model_kwargs={\"temperature\": 0.01, \"top_p\": 1, \"max_new_tokens\": 1000}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbbfbc6-e784-4cc9-8bd8-7f11e16c9456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
